{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Wrangle OpenStreetMaps Data\n",
    "#### Mahlon Barrault\n",
    "#### August 28, 2015\n",
    "#### Map Area: Calgary, Alberta (Map Zen Extract (https://mapzen.com/data/metro-extracts) includes suburbs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Problems Encountered in the Map](#Problems-Encountered-in-the-Map)\n",
    "    \n",
    "* [Directional Suffixes and Street Names](#Directional-Suffixes-and-Street-Names)\n",
    "* [Postal Codes](#Postal-Codes)\n",
    "* [Rural Roads](#Rural-Roads)\n",
    "\n",
    "[Data Overview](#Data-Overview)\n",
    "\n",
    "[Additional Ideas](#Additional-Ideas)\n",
    "\n",
    "[Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I chose to use the map data from the area in which I live, Calgary, Alberta, Canada. Being a local I have and an advantage in domain knowledge. Leveraging information I have acquired from living here I was able to produce more accurate standardizations. One example, which you will read more about later, was directional suffixes. In the data they appeared in a wide range of formats, however on signage in Calgary a direction initialism is used without punctuation. I invite you to read more about my process in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered in the Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the outset of the this project I was interested in loading as much data in to MongoDB as was possible from the MapZen extract of Calgary. My motivation for this was to not stiffle any analyical possiblities later in the process because of not having the right data at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag element in the extract file contained k and v attributes that I wanted to unwind and import in to MongoDB. I used the following regular expression to find any keys with problematic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This regex represents invalid MongoDB characters for keys\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No keys with problem characters were found. Next I wanted to see what sort of tags and attribute I would be dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags:\n",
      "defaultdict(<type 'int'>, {'node': 779009, 'nd': 935489, 'bounds': 1, 'member': 22520, 'tag': 325867, 'relation': 537, 'way': 83492, 'osm': 1})\n",
      "\n",
      "Attributes:\n",
      "defaultdict(<type 'int'>, {'changeset': 863038, 'maxlon': 1, 'type': 22520, 'uid': 863038, 'generator': 1, 'timestamp': 863039, 'k': 325867, 'v': 325867, 'lon': 779009, 'minlat': 1, 'version': 863039, 'role': 22520, 'user': 863038, 'maxlat': 1, 'lat': 779009, 'ref': 958009, 'id': 863038, 'minlon': 1})\n"
     ]
    }
   ],
   "source": [
    "%run mapparser.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these attributes were expected. The 'type' keys would have conflicted with the 'type' key that was going to be used to represent the tag names. I took note of this so later the key that would have the tag name could be renamed to 'node_type'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directional Suffixes and Street Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I was sufficiently confident that I would be able to translate the tag and attributes from the extract file in to well formed MongoDB documents, it was time to turn my attention to the deeper contents in the attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ideally formatted Calgary address looks like 1234 StreetName StreetType DirectionSuffix. I wrote the following helper functions to help me disassemble the 'addr:street' key and get a sense for what other problems might exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_street_type(street_types, street_name):\n",
    "    ''' Modfies street_types dict. Adds street_name to dict if it does not have\n",
    "    a conformed street type or conformed directional suffix.\n",
    "    '''\n",
    "    m = get_suffix(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in ST_TYPE_EXPECTED:\n",
    "                street_types[street_type].add(street_name)\n",
    "\n",
    "def get_suffix(street_name):\n",
    "    ''' Returns the last word from street_name with any directional suffixes\n",
    "    removed.\n",
    "    ''' \n",
    "    return street_type_re.search(street_type_re.sub(remove_dir, street_name))\n",
    "\n",
    "def remove_dir(m):\n",
    "    ''' Accepts a RegEx match. Returns and empty string if the match is an\n",
    "    expected directional suffix. Returns the whole match if it is not a \n",
    "    directional suffix.\n",
    "    '''\n",
    "    if m.group().upper() in DIR_EXPECTED:\n",
    "        return ''\n",
    "    else:\n",
    "        return m.group()\n",
    "    return\n",
    "\n",
    "def process_match(m):\n",
    "    ''' Returns the value from ST_TYPE_MAPPING with corresponding key from RegEx\n",
    "    search. If no key exists the orginal search results from m are returned \n",
    "    unmodified.\n",
    "    '''\n",
    "    if ST_TYPE_MAPPING.get(m.group()) != None:\n",
    "        return ST_TYPE_MAPPING.get(m.group())\n",
    "    else:\n",
    "        return m.group()\n",
    "    return \n",
    "\n",
    "\n",
    "def update_name(name):\n",
    "    ''' Returns conformed name if there was a mapping in ST_TYPE_MAPPING.\n",
    "    Otherwise returns name.\n",
    "    '''\n",
    "    return street_type_re.sub(process_match, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found several addresses that were not ideally formatted. Here is a sample of some of the problems that were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_problems = {'Northeast': set(['16 Avenue Northeast']),'West': set(['Pine Ridge Road West', '206 Avenue West']), \\\n",
    " 'Cres': set(['Bracewood Cres']), 'N.W': set(['Symons Valley Rd N.W']), 'South-west': set(['Everhollow Manor  South-west']), \\\n",
    " 'Alberta': set(['Veterans Blvd., Airdrie, Alberta']), 'AB': set(['32nd Street, Okotoks, AB']), \\\n",
    " 'Blvd.': set(['Vetrans Blvd.']), 'street': set(['2 street'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the directional suffixes I saw that the full words were used as in 'Northeast'. Sometimes they were hyphenated as with 'South-west' or they were an initialism but with punctuation separators like 'N.W'. With the street types there were abbreviations, abbreviations with punctuation. Sometimes the province had been included in the address, so that would need to be accomidated. The last example in the dictionary is an example of an street without a directional suffix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with a exhaustive list of all of the directional suffix and street name problems I could go to work cleaning them. At the heart of the cleaning operation was the following recursive (and well documented) function. I choose to over-comment this function because recursion can bend your mind sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_st_name(street_name):\n",
    "    ''' Recursive function to conform street types and directional suffixes'''\n",
    "    street_name = street_name.split(',')[0]\n",
    "\n",
    "    init_search = street_type_re.search(street_name)\n",
    "    \n",
    "    if init_search:\n",
    "        init_search = init_search.group()\n",
    "        \n",
    "        if init_search in DIR_EXPECTED:\n",
    "            # street_name should now have the street type exposed on the end\n",
    "            # of the string\n",
    "            street_name = street_type_re.sub(remove_suffix, street_name)\n",
    "            # Recusive call to have the funtion check street type. strip() is\n",
    "            # needed so that the RegEx matches properly\n",
    "            return str(update_st_name(street_name.strip())) + ' ' + init_search\n",
    "        elif init_search in DIR_MAPPING:\n",
    "            dir_clean = DIR_MAPPING[init_search]\n",
    "            street_name = street_type_re.sub(remove_suffix, street_name)\n",
    "            # Recusive call to have the funtion check street type. strip() is\n",
    "            # needed so that the RegEx matches properly\n",
    "            return str(update_st_name(street_name.strip())) + ' ' + dir_clean\n",
    "        elif init_search in ST_TYPE_EXPECTED:\n",
    "            # Recursive base case\n",
    "            return street_name\n",
    "        elif init_search in ST_TYPE_MAPPING:\n",
    "            st_ty_clean = ST_TYPE_MAPPING[init_search]\n",
    "            street_name = street_type_re.sub(st_ty_clean, street_name)\n",
    "            # Alternate recursive base case\n",
    "            return street_name\n",
    "        else:\n",
    "            # Catches streets that will not be cleaned like \n",
    "            # 'Township Road  204A'\n",
    "            return street_name\n",
    "    else:\n",
    "        # Recursion should not hit this else, something has gone wrong; will\n",
    "        # return 'None'\n",
    "        return street_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job of this function was to take apart the incoming address and reassemble it with a conformed directional suffix and street type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_st_name('16 Avenue Northeast')\n",
    "'16 Avenue NE'\n",
    "update_st_name('Pine Ridge Road West')\n",
    "'Pine Ridge Road W'\n",
    "update_st_name('Bracewood Cres')\n",
    "'Bracewood Crescent'\n",
    "update_st_name('Symons Valley Rd N.W')\n",
    "'Symons Valley Road NW'\n",
    "update_st_name('Everhollow Manor  South-west')\n",
    "'Everhollow Manor SW'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postal Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some postal codes were not in the official format A1B 2C3. The bad postcodes where audited in post_import_audit.py. There were 49 malformed postcodes, 33 of them being unique. A defaultdict was used to produce a dict with keys of the malformed postal codes with blank values. This dict was printed and copied in to the editor and the corrections were done manually since there were an manageable number. This took about 5 minutes. This dictionary with the now correct values was moved to post_import_clean.py and a new function was created to do the actual updates on my MongoDB instance. After running the cleaning file again, the post_import_audit.py was run again and the query to check for malformed postcodes returned no results.\n",
    "\n",
    "Based on feedback regarding cleaning post codes it seemed like programmatically cleaning post codes was a more scalable solution. Most of the problems with the post codes were either lower case letters or missing segment separator (a space at character 3 in the post code string). A RegEx was used that would identify all of the post codes that were close but needed minor string modification. Each match was converted to upper case and evaluated for a space at character 3. If the total length was 6 or less than a space was inserted into the post code. If the length was greater than 6 (indicating there was a segment separator but it was not a space) then character 3 was replaced with a space. All malformed post codes that did not have the basic post code form (i.e. phone numbers in post code field) were replaced with an empty sting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rural Roads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset encompassed a very large area and included large parts of farm area around the City of Calgary. As a result there are several rural roads like 'Township Road 204A' which aren't strictly part of the city. Their inclusion did not cause significant issues but they do not add much value to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File Sizes\n",
    "calgary_canada.osm : 159 MB\n",
    "\n",
    "calgary_canada.osm.json : 184 MB\n",
    "\n",
    "##### Number of Documents\n",
    "    db.DANDP3.count() : 863038\n",
    "\n",
    "###### See analyze.py for full code for the following\n",
    "\n",
    "##### Largest Document\n",
    "    print get_largest_doc(get_all_docs(db)) : 112886 characters, 'Proposed West Stoney Trail'\n",
    "\n",
    "##### Number of Unique Users\n",
    "    print len(get_users(docs)) : 767\n",
    "\n",
    "##### Top Three Contributors\n",
    "    print users[0:3] : [{u'count': 309818, u'_id': u'sbrown'}, {u'count': 89769, u'_id': u'Zippanova'}, {u'count': 46296, u'_id': u'markbegbie'}]\n",
    "\n",
    "##### Rank of My Contributions\n",
    "    print 'My Rank ' + mb_rank : 157\n",
    "\n",
    "##### Number of Ways\n",
    "    print 'Number of Ways: ' + str(db.DANDP3.find({\"node_type\":\"way\"}).count()) : 83492\n",
    "\n",
    "##### Number of Nodes\n",
    "    print 'Number of Nodes: ' + str(db.DANDP3.find({\"node_type\":\"node\"}).count()) : 779009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postal Code Standarization\n",
    "During my post import cleaning I made several standardization corrections to the postcodes in the dataset. However, even the correctly formatted postcodes may not be legitimate postcodes. In order to accomplish this, I would have to develop functions that could communicate with the Canada Post address validation API. This would allow me to fully validate whole addresses including the postcodes and then take corrective action. The Canada Post API has the capability to suggest addresses that are closely matching the query submitted. This might pose an interesting challenge to do this autonomously without user intervention. From examining the Canada Post API it seems that it is designed to validate whole addresses. To utilize this API additional effort in to auditing and cleaning the other address components would be needed. Many of the nodes in the dataset are for ambiguous areas or objects. Some of these may not have real world physical address so full address conformation may not be possible for many nodes. From the information supplied on some nodes there may be several matching addresses for that node. A strategy would have to be implemented for picking the most appropriate address for each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze User Contributions by Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought it might be interesting to analyze what areas of the city users tend to make contributions. Are there patterns in user contribution that would indicate where that users lives or works? Are there more contributions in newer areas of the city or is it evenly distributed? How much does the City of Calgary contribute? These are just a few questions that I would be interested in digging in to further. Data visualization could be used to get a basic idea of clusters of contribution activity. To get the most satisfying answers machine learning would have to be implemented to expose these correlations in the data. It would be challenging to accurately isolate each area of the city to analyze the user contribution activity. Data from the city's website would need to be used to enrich the OpenStreetMaps data including neighborhood boundaries, city wards and demographics. These data would likely need a similar munging treatment to be usable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Additional Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many places in Calgary accept bitcoin?\n",
    "    len(list(db.DANDP3.find({\"payment:bitcoin\" : \"yes\"}))) : 18 \n",
    "\n",
    "##### What are the most common amenties?\n",
    "    amenites = list(aggregate(db, [{'$match' : {'amenity' : {'$exists' : 1}}},\\\n",
    "                               {'$group' : {'_id' : '$amenity', 'count' : \\\n",
    "                               {'$sum' : 1}}}, {'$sort' : {'count' : -1}}]))\n",
    "    print amenites[0:3]\n",
    "    [{u'count': 1735, u'_id': u'parking'}, {u'count': 461, u'_id': u'restaurant'}, {u'count': 441, u'_id': u'fast_food'}]\n",
    "\n",
    "##### What are the most common fast food places?\n",
    "\n",
    "    fast_food_by_name = list(aggregate(db, [{'$match' : {'amenity' : {'$exists' : 1}, 'name' : {'$exists' : 1} }}, {'$group' : {'_id' : '$name', 'count' : {'$sum' : 1}}},{'$sort' : {'count' : -1}}]))\n",
    "    print fast_food_by_name[0:3]\n",
    "    \n",
    "     [{u'_id': u'Tim Hortons', u'count': 59}, {u'_id': u'Subway', u'count': 54}, {u'_id': u'Shell', u'count': 36}]\n",
    "     \n",
    "##### Additional additonal analysis can be found in analyze.py starting at line 111."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working on the Additional Analysis section I discovered that there were some keys that I would have liked to take a closer look but their structure (key:sub_key) made it difficult to get good grouping. This solidified the lesson of audit, clean, repeat for me. Because of the nature of this issue I would have had to go back to the original file processing code and add logic to turn the key:sub_key entries to key : [sub_key : value] entries. For the sake of time I chose not to do this for this project but I want to acknowledge that despite my efforts in cleaning the dataset there is still much more that can be done to improve it. \n",
    "\n",
    "Based on the vast number of contributors, it would seem that people are interested in keeping the map data for Calgary up-to-date. This has advantages and disadvantages for the quality of the data. The more users that contribute the more ways data could be entered incorrectly. Understanding that OpenStreetMaps endeavors to be open, it might be advantageous to intelligently offer suggestions as someone is entering data. This might reduce the number of ill-conceived keys in the data and improve the analysis of map data. It has been shown in many other open systems that intelligent suggestions are not always accepted or appreciated. Perhaps there should be some penalty for creating new keys like having to submit a request to a moderator to have the key added to an object. This might influence contributor behavior to accept suggestions or reconsider if a new key is really necessary. However, this could also have unintended consequences. User frustration could increase and user contributions could suffer. Users might begin to store information on keys where it would not be meaningful or relevant to do so.\n",
    "\n",
    "While working with OpenStreetMaps data I reflected on what an undertaking it would be to consume these data to enrich a service like Google Maps. A pipeline to ingest these data would have to account for many of the factors that I had to consider plus many more. I have only examined a small fraction of the OpenStreeMaps data and there was a wide range of data quality challenges. I would be interested to see if the techniques I employed are similar to engineers who work on systems that consider ALL OpenStreetMaps data. No doubt, as we produce more and more data as a society we will need to develop systems that assist the data wrangler. For now, we will just have to keep auditing, cleaning, and repeating."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
