{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Wrangle OpenStreetMaps Data\n",
    "#### Mahlon Barrault\n",
    "#### August 28, 2015\n",
    "#### Map Area: Calgary, Alberta (Map Zen Extract (https://mapzen.com/data/metro-extracts) includes suburbs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Problems Encountered in the Map](#Problems-Encountered-in-the-Map)\n",
    "    \n",
    "* [Directional Suffixes and Street Names](#Directional-Suffixes-and-Street-Names)\n",
    "* [Postal Codes](#Postal-Codes)\n",
    "* [Rural Roads](#Rural-Roads)\n",
    "\n",
    "[Data Overview](#Data-Overview)\n",
    "\n",
    "[Additional Ideas](#Additional-Ideas)\n",
    "\n",
    "[Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I chose to use the map data from the area in which I live, Calgary, Alberta, Canada. Being a local I have an advantage in domain knowledge. Leveraging information I have acquired from living here I was able to produce more accurate standardizations. One example, which you will read more about later, was directional suffixes. In the data they appeared in a wide range of formats, however on signage in Calgary a direction initialism is used without punctuation. I invite you to read more about my process in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered in the Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the outset of this project I was interested in loading as much data into MongoDB as was possible from the MapZen extract of Calgary. My motivation for this was to not stiffle any analyical possiblities later in the process because of not having the right data at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag element in the extract file contained k and v attributes that I wanted to unwind and import in to MongoDB. I used the following regular expression to find any keys with problematic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# This regex represents invalid MongoDB characters for keys\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No keys with problem characters were found. Next I wanted to see what sort of tags and attribute I would be dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run mapparser.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these attributes were expected. The 'type' keys would have conflicted with the 'type' key that was going to be used to represent the tag names. I took note of this so later the key that would have the tag name could be renamed to 'node_type'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directional Suffixes and Street Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I was sufficiently confident that I would be able to translate the tag and attributes from the extract file into well formed MongoDB documents, it was time to turn my attention to the deeper contents in the attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ideally formatted Calgary address looks like 1234 StreetName StreetType DirectionSuffix. I wrote the following helper functions to help me disassemble the 'addr:street' key and get a sense for what other problems might exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_street_type(street_types, street_name):\n",
    "    ''' Modfies street_types dict. Adds street_name to dict if it does not have\n",
    "    a conformed street type or conformed directional suffix.\n",
    "    '''\n",
    "    m = get_suffix(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in ST_TYPE_EXPECTED:\n",
    "                street_types[street_type].add(street_name)\n",
    "\n",
    "def get_suffix(street_name):\n",
    "    ''' Returns the last word from street_name with any directional suffixes\n",
    "    removed.\n",
    "    ''' \n",
    "    return street_type_re.search(street_type_re.sub(remove_dir, street_name))\n",
    "\n",
    "def remove_dir(m):\n",
    "    ''' Accepts a RegEx match. Returns and empty string if the match is an\n",
    "    expected directional suffix. Returns the whole match if it is not a \n",
    "    directional suffix.\n",
    "    '''\n",
    "    if m.group().upper() in DIR_EXPECTED:\n",
    "        return ''\n",
    "    else:\n",
    "        return m.group()\n",
    "    return\n",
    "\n",
    "def process_match(m):\n",
    "    ''' Returns the value from ST_TYPE_MAPPING with corresponding key from RegEx\n",
    "    search. If no key exists the orginal search results from m are returned \n",
    "    unmodified.\n",
    "    '''\n",
    "    if ST_TYPE_MAPPING.get(m.group()) != None:\n",
    "        return ST_TYPE_MAPPING.get(m.group())\n",
    "    else:\n",
    "        return m.group()\n",
    "    return \n",
    "\n",
    "\n",
    "def update_name(name):\n",
    "    ''' Returns conformed name if there was a mapping in ST_TYPE_MAPPING.\n",
    "    Otherwise returns name.\n",
    "    '''\n",
    "    return street_type_re.sub(process_match, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found several addresses that were not ideally formatted. Here is a sample of some of the problems that were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_problems = {'Northeast': set(['16 Avenue Northeast']),'West': set(['Pine Ridge Road West', '206 Avenue West']), \\\n",
    " 'Cres': set(['Bracewood Cres']), 'N.W': set(['Symons Valley Rd N.W']), 'South-west': set(['Everhollow Manor  South-west']), \\\n",
    " 'Alberta': set(['Veterans Blvd., Airdrie, Alberta']), 'AB': set(['32nd Street, Okotoks, AB']), \\\n",
    " 'Blvd.': set(['Vetrans Blvd.']), 'street': set(['2 street'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the directional suffixes I saw that the full words were used as in 'Northeast'. Sometimes they were hyphenated as with 'South-west' or they were an initialism but with punctuation separators like 'N.W'. With the street types there were abbreviations, abbreviations with punctuation. Sometimes the province had been included in the address, so that would need to be accommodated. The last example in the dictionary is an example of a street without a directional suffix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with a exhaustive list of all of the directional suffix and street name problems I could go to work cleaning them. At the heart of the cleaning operation was the following recursive (and well documented) function. I choose to over-comment this function because recursion can bend your mind sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_st_name(street_name):\n",
    "    ''' Recursive function to conform street types and directional suffixes'''\n",
    "    street_name = street_name.split(',')[0]\n",
    "\n",
    "    init_search = street_type_re.search(street_name)\n",
    "    \n",
    "    if init_search:\n",
    "        init_search = init_search.group()\n",
    "        \n",
    "        if init_search in DIR_EXPECTED:\n",
    "            # street_name should now have the street type exposed on the end\n",
    "            # of the string\n",
    "            street_name = street_type_re.sub(remove_suffix, street_name)\n",
    "            # Recusive call to have the funtion check street type. strip() is\n",
    "            # needed so that the RegEx matches properly\n",
    "            return str(update_st_name(street_name.strip())) + ' ' + init_search\n",
    "        elif init_search in DIR_MAPPING:\n",
    "            dir_clean = DIR_MAPPING[init_search]\n",
    "            street_name = street_type_re.sub(remove_suffix, street_name)\n",
    "            # Recusive call to have the funtion check street type. strip() is\n",
    "            # needed so that the RegEx matches properly\n",
    "            return str(update_st_name(street_name.strip())) + ' ' + dir_clean\n",
    "        elif init_search in ST_TYPE_EXPECTED:\n",
    "            # Recursive base case\n",
    "            return street_name\n",
    "        elif init_search in ST_TYPE_MAPPING:\n",
    "            st_ty_clean = ST_TYPE_MAPPING[init_search]\n",
    "            street_name = street_type_re.sub(st_ty_clean, street_name)\n",
    "            # Alternate recursive base case\n",
    "            return street_name\n",
    "        else:\n",
    "            # Catches streets that will not be cleaned like \n",
    "            # 'Township Road  204A'\n",
    "            return street_name\n",
    "    else:\n",
    "        # Recursion should not hit this else, something has gone wrong; will\n",
    "        # return 'None'\n",
    "        return street_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job of this function was to take apart the incoming address and reassemble it with a conformed directional suffix and street type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "update_st_name('16 Avenue Northeast')\n",
    "'16 Avenue NE'\n",
    "update_st_name('Pine Ridge Road West')\n",
    "'Pine Ridge Road W'\n",
    "update_st_name('Bracewood Cres')\n",
    "'Bracewood Crescent'\n",
    "update_st_name('Symons Valley Rd N.W')\n",
    "'Symons Valley Road NW'\n",
    "update_st_name('Everhollow Manor  South-west')#\n",
    "'Everhollow Manor SW'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the basic tests above we can see update_st_name() is doing a great job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postal Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Canada there is wide spread confusion regarding how postal codes are to be written. According to Canada Post's guidelines: \"The Postal CodeOM should be printed in upper case with the first three characters separated from the last three by one space. Do not use hyphens.\" [<sup>1</sup>](#References). This will be the standard I will conform postal codes in the data set to.\n",
    "\n",
    "Altogether there were 49 improperly formatted codes 33 of which were unique. Initially I wanted to clean these values by simply creating a defaultdict map between the malformed keys and the set of manually conformed keys. Despite being a manageable number of changes to handle manually, creating defaultdicts by hand is not a particularly scalable technique. Instead I created the following function to programmatically generate the defaultdict which could be used to update a collection in MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_post_codes(db):\n",
    "    '''\n",
    "    Accepts a mongodb instance, finds and cleans malformed postal codes in the\n",
    "    provided db. Assumes the database has a collection called DANDP3.\n",
    "    '''\n",
    "    post_code_changes = defaultdict(set)\n",
    "    bad_post_codes = []\n",
    "\n",
    "    # Finds strings that have the correct letter number alternation but might not\n",
    "    # have the correct segment separator\n",
    "    re_valid_post_code = re.compile('[A-Za-z]\\d[A-Za-z]( ?\\-?)\\d[A-Za-z]\\d')\n",
    "    \n",
    "    # Retreive nonconformed post codes from mongodb\n",
    "    bad_postcode_docs = db.DANDP3.find({'address.postcode' : {'$exists': 'true', \\\n",
    "    '$not' : re.compile('^([A-Z]\\d[A-Z]( )\\d[A-Z]\\d)')}})\n",
    "    \n",
    "    for d in bad_postcode_docs:\n",
    "        bad_post_codes.append(d)\n",
    "        \n",
    "    for bpc in bad_post_codes:\n",
    "        match = re_valid_post_code.search(bpc)\n",
    "        \n",
    "        if match:\n",
    "            post_code = match.group()\n",
    "            post_code = post_code.upper()\n",
    "            \n",
    "            if post_code[3] != ' ':\n",
    "                if len(post_code) <= 6:\n",
    "                    # Insert a space if it is missing\n",
    "                    post_code = post_code[0:3] + ' ' + post_code[3:6]\n",
    "                elif len(post_code) > 6:\n",
    "                    # post code len is good but char 3 is not a space, replace it\n",
    "                    post_code = post_code.replace(post_code[3], ' ')\n",
    "            post_code_changes[bpc] = post_code\n",
    "        else:\n",
    "            post_code_changes[bpc] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean_post_codes uses two slightly different regular expressions: one to find all postal code-like values and another to find which of them are malformed. The rest of the function used string slices and built-in string methods to manipulate the postal code in to submission. Any values that bear little to no resemblance to a postal code are set to an empty string. After running this function, the defaultdict post_code_changes looks like this:\n",
    "\n",
    "```\n",
    "defaultdict(set,\n",
    "            {' T2J 0P8': 'T2J 0P8',\n",
    "             '1212': '',\n",
    "             '403-719-6250': '',\n",
    "             'AB T2G 2L2': 'T2G 2L2',\n",
    "             'AB T2S 2N1': 'T2S 2N1',\n",
    "             'T1X1L8': 'T1X 1L8',\n",
    "             'T2E': '',\n",
    "             'T2G0H7': 'T2G 0H7',\n",
    "             'T2J2T8': 'T2J 2T8',\n",
    "             'T2L1G1': 'T2L 1G1',\n",
    "             'T2P0W3': 'T2P 0W3',\n",
    "             'T2P3P8': 'T2P 3P8',\n",
    "             'T2R0E7': 'T2R 0E7',\n",
    "             'T2T0A7;T2T 0A7': 'T2T 0A7',\n",
    "             'T2V2X3': 'T2V 2X3',\n",
    "             'T3A0H7': 'T3A 0H7',\n",
    "             'T3A5R8': 'T3A 5R8',\n",
    "             'T3A6J1': 'T3A 6J1',\n",
    "             'T3B3X3': 'T3B 3X3',\n",
    "             'T3G2V7': 'T3G 2V7',\n",
    "             'T3J0G7': 'T3J 0G7',\n",
    "             'T3J0S3': 'T3J 0S3',\n",
    "             'T3J4L8': 'T3J 4L8',\n",
    "             'T3K-5P4': 'T3K 5P4',\n",
    "             'T3N0A6': 'T3N 0A6',\n",
    "             'T3N0E4': 'T3N 0E4',\n",
    "             'T3R0A1': 'T3R 0A1',\n",
    "             'T3R0H3': 'T3R 0H3',\n",
    "             'T3a4b3': 'T3A 4B3',\n",
    "             't2n 3P3': 'T2N 3P3',\n",
    "             't2n 4l7': 'T2N 4L7',\n",
    "             't3G 5T3': 'T3G 5T3',\n",
    "             't3c2h6': 'T3C 2H6'})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples above we can see that the function is handling the incoming values appropriately and we are left with a clean conformed set of postal codes! This dictionary can now be used to update MongoDB or do something else like get additional metadata about the postal code from another database or API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rural Roads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset encompassed a very large area and included large parts of farm area around the City of Calgary. As a result there are several rural roads like 'Township Road 204A' which aren't strictly part of the city. Their inclusion did not cause significant issues but they do not add much value to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File Sizes\n",
    "calgary_canada.osm : 159 MB\n",
    "\n",
    "calgary_canada.osm.json : 184 MB\n",
    "\n",
    "##### Number of Documents\n",
    "    db.DANDP3.count() : 863038\n",
    "\n",
    "###### See analyze.py for full code for the following\n",
    "\n",
    "##### Largest Document\n",
    "    print get_largest_doc(get_all_docs(db)) : 112886 characters, 'Proposed West Stoney Trail'\n",
    "\n",
    "##### Number of Unique Users\n",
    "    print len(get_users(docs)) : 767\n",
    "\n",
    "##### Top Three Contributors\n",
    "    print users[0:3] : [{u'count': 309818, u'_id': u'sbrown'}, {u'count': 89769, u'_id': u'Zippanova'}, {u'count': 46296, u'_id': u'markbegbie'}]\n",
    "\n",
    "##### Rank of My Contributions\n",
    "    print 'My Rank ' + mb_rank : 157\n",
    "\n",
    "##### Number of Ways\n",
    "    print 'Number of Ways: ' + str(db.DANDP3.find({\"node_type\":\"way\"}).count()) : 83492\n",
    "\n",
    "##### Number of Nodes\n",
    "    print 'Number of Nodes: ' + str(db.DANDP3.find({\"node_type\":\"node\"}).count()) : 779009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postal Code Standarization\n",
    "During my post import cleaning I made several standardization corrections to the postcodes in the dataset. However, even the correctly formatted postcodes may not be legitimate postcodes. I thought that a next step would be to connect to Canada Post's address validation API and confirm that the postal codes exist. This would allow me to collect some additional metadata regarding what are each valid postal code covers. This could be compared to the coordinates of the object from the map data and further validate the postal code and possibly suggest a correct postal code based on the coordinates.\n",
    "\n",
    "I would have to develop functions that could communicate with the Canada Post address validation API. This would allow me to fully validate whole addresses including the postcodes and then take corrective action. The Canada Post API has the capability to suggest addresses that are closely matching the query submitted. \n",
    "\n",
    "From examining the Canada Post API it seems that it is designed to validate whole addresses and not necessarily the components of the address. To utilize this API additional effort into auditing and cleaning the other address components would be needed. Many of the nodes in the dataset are for ambiguous areas or objects. Some of these may not have real world physical addresses so full address conformation may not be possible for many nodes. From the information supplied on some nodes there may be several matching addresses for that node. A strategy would have to be implemented for picking the most appropriate address for each node.\n",
    "\n",
    "This might pose an interesting challenge to do this autonomously without user intervention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze User Contributions by Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another potential analysis might be to examine what areas of the city users tend to make contributions. Are there patterns in user contribution that would indicate where that users lives or works? Are there more contributions in newer areas of the city or is it evenly distributed? How much does the City of Calgary contribute? These are just a few questions that I would be interested in digging into further. Data visualization could be used to get a basic idea of clusters of contribution activity. \n",
    "\n",
    "To get the most satisfying answers machine learning would have to be implemented to expose these correlations in the data. It would be challenging to accurately isolate each area of the city to analyze the user contribution activity. Data from the city's website would need to be used to enrich the OpenStreetMaps data including neighborhood boundaries, city wards and demographics. These data would likely need a similar munging treatment to be usable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Additional Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many places in Calgary accept bitcoin?\n",
    "    len(list(db.DANDP3.find({\"payment:bitcoin\" : \"yes\"}))) : 18 \n",
    "\n",
    "##### What are the most common amenties?\n",
    "    amenites = list(aggregate(db, [{'$match' : {'amenity' : {'$exists' : 1}}},\\\n",
    "                               {'$group' : {'_id' : '$amenity', 'count' : \\\n",
    "                               {'$sum' : 1}}}, {'$sort' : {'count' : -1}}]))\n",
    "    print amenites[0:3]\n",
    "    [{u'count': 1735, u'_id': u'parking'}, {u'count': 461, u'_id': u'restaurant'}, {u'count': 441, u'_id': u'fast_food'}]\n",
    "\n",
    "##### What are the most common fast food places?\n",
    "\n",
    "    fast_food_by_name = list(aggregate(db, [{'$match' : {'amenity' : {'$exists' : 1}, 'name' : {'$exists' : 1} }}, {'$group' : {'_id' : '$name', 'count' : {'$sum' : 1}}},{'$sort' : {'count' : -1}}]))\n",
    "    print fast_food_by_name[0:3]\n",
    "    \n",
    "     [{u'_id': u'Tim Hortons', u'count': 59}, {u'_id': u'Subway', u'count': 54}, {u'_id': u'Shell', u'count': 36}]\n",
    "     \n",
    "##### Additional additonal analysis can be found in analyze.py starting at line 111."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working on the Additional Analysis section I discovered that there were some keys that I would have liked to take a closer look but their structure (key:sub_key) made it difficult to get good grouping. This solidified the lesson of audit, clean, repeat for me. Because of the nature of this issue I would have had to go back to the original file processing code and add logic to turn the key:sub_key entries to key : [sub_key : value] entries. Despite my efforts in cleaning the dataset there is still much more that can be done to improve it. \n",
    "\n",
    "Based on the vast number of contributors, it would seem that people are interested in keeping the map data for Calgary up-to-date. This has advantages and disadvantages for the quality of the data. The more users that contribute the more ways data could be entered incorrectly. \n",
    "\n",
    "Understanding that OpenStreetMaps endeavors to be open, it might be advantageous to intelligently offer suggestions as someone is entering data. This might reduce the number of ill-conceived keys in the data and improve the analysis of map data. It has been shown in many other open systems that intelligent suggestions are not always accepted or appreciated. Perhaps there should be some penalty for creating new keys like having to submit a request to a moderator to have the key added to an object. This might influence contributor behavior to accept suggestions or reconsider if a new key is really necessary. However, this could also have unintended consequences. User frustration could increase and user contributions could suffer. Users might begin to store information on keys where it would not be meaningful or relevant to do so.\n",
    "\n",
    "While working with OpenStreetMaps data I reflected on what an undertaking it would be to consume these data to enrich a service like Google Maps. A pipeline to ingest these data would have to account for many of the factors that I had to consider plus many more. I have only examined a small fraction of the OpenStreeMaps data and there was a wide range of data quality challenges. I would be interested to see if the techniques I employed are similar to engineers who work on systems that consider ALL OpenStreetMaps data. No doubt, as we produce more and more data as a society we will need to develop systems that assist the data wrangler. For now, we will just have to keep auditing, cleaning, and repeating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Canada Post. (2016). Addressing Guidelines | Canada Post. [online] Canadapost.ca. Available at: https://www.canadapost.ca/tools/pg/manual/PGaddress-e.asp?ecid=murl10006450 [Accessed 19 Nov. 2016]."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
