{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangle OpenStreetMaps Data\n",
    "#### Mahlon Barrault\n",
    "#### August 28, 2015\n",
    "#### Map Area: Calgary, Alberta (Map Zen Extract (https://mapzen.com/data/metro-extracts) includes suburbs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Problems Encountered in the Map](#Problems-Encountered-in-the-Map)\n",
    "    \n",
    "* [Directional Suffixes](#Directional-Suffixes)\n",
    "* [Postal Codes](#Postal-Codes)\n",
    "* [Rural Roads](#Rural-Roads)\n",
    "\n",
    "[Data Overview](#Data-Overview)\n",
    "\n",
    "[Additional Ideas](#Additional-Ideas)\n",
    "\n",
    "[Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered in the Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%run tags.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No characters that could cause issues creating keys in MongoDB were discovered. However, there appear to be some keys that are inconsistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags:\n",
      "defaultdict(<type 'int'>, {'node': 779009, 'nd': 935489, 'bounds': 1, 'member': 22520, 'tag': 325867, 'relation': 537, 'way': 83492, 'osm': 1})\n",
      "\n",
      "Attributes:\n",
      "defaultdict(<type 'int'>, {'changeset': 863038, 'maxlon': 1, 'type': 22520, 'uid': 863038, 'generator': 1, 'timestamp': 863039, 'k': 325867, 'v': 325867, 'lon': 779009, 'minlat': 1, 'version': 863039, 'role': 22520, 'user': 863038, 'maxlat': 1, 'lat': 779009, 'ref': 958009, 'id': 863038, 'minlon': 1})\n"
     ]
    }
   ],
   "source": [
    "%run mapparser.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these attributes were expected. 'type' keys would have conflicted with the 'type' key that the Lesson 6 code was adding to the documents. This key was renamed to node_type. 'role' was not expected. It belonged to 'member' tags. More on processing 'member' tags to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%run audit.py\n",
    "\n",
    "DIR_MAPPING = {'East' : 'E',\n",
    "               'N.E.' : 'NE',\n",
    "               'N.E' : 'NE',\n",
    "               'N.W' : 'NW',\n",
    "               'N.W.' : 'NW',\n",
    "               'North' : 'N',\n",
    "               'Northeast' : 'NE',\n",
    "               'Northwest' : 'NW',\n",
    "               'S.E' : 'SE',\n",
    "               'S.E.' : 'SE',\n",
    "               'S.W' : 'SW',\n",
    "               'S.W.' : 'SW',\n",
    "               'South' : 'S',\n",
    "               'South-west' : 'SW',\n",
    "               'Southeast' : 'SE',\n",
    "               'South-east' : 'SE',\n",
    "               'Southwest' : 'SW',\n",
    "               'West' : 'W'\n",
    "               }\n",
    "\n",
    "ST_TYPE_MAPPING = { \"St\": \"Street\",\n",
    "                   \"St.\": \"Street\",\n",
    "                   'street' : 'Street',\n",
    "                   \"Rd.\" : 'Road',\n",
    "                   \"Rd\" : 'Road',\n",
    "                   'Ave' : \"Avenue\",\n",
    "                   'Ave.' : \"Avenue\",\n",
    "                   'Cres' : 'Crescent',\n",
    "                   'Cres.' : 'Crescent',\n",
    "                   'Blvd' : 'Boulevard',\n",
    "                   'Blvd.' : 'Boulevard'\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of audit.py allowed DIR_MAPPING and ST_TYPE_MAPPING to be produced which was used to clean the directional suffixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an explanation of my process of creating clean.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directional Suffixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the audit of street names the functions for the audit from Lesson 6 were altered to compensate for the use of directional suffixes. The standard for this notation was chosen to be the initials of the directional suffix, since that is the notation that is used on street signs in Calgary. Some of the 'addr:street' values have city and province included, so those data were split on ',' and the first item was used. Values like '400123 Highway 66' and 'Township Road  204A' were not altered as they are valid as they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a function in audit.py to get a count of tags using a specific attribute. member tags with role attributes were discovered that needed to be compensated for in the design of the cleaning functions. There were several 'type' attributes that would conflict with the 'type' key used in shape_node(), so it was renamed to 'node_type'. To assist in building the structure of the tags that had 'member' children member_prototype.json was produced to help me visualize what it should look like. From that I was able to correctly code the section in shape_node for member tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While working on the street name cleaning function to compensate for the street directions the trailing white space was causing the regex to not find the suffixes. Added strip() to the calls to the RegEx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial test after developing clean.py revealed that the \"tag\" tags were getting processed at the top level shape_base as well as shape_node. The condition on the call to shape_base in shape_element will need to be recoded and include the relation tags. There are \"tag\" tags that have a created_by k value, these need to be added to the created dictonary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the cleaned data was imported the audit functions were executed against the data now in MongoDB. There were several None values detected. Examination of the update_st_name function determined that there was a corner case that was not accounted for. There were some street values like 'Township Road  204A' that were not a concern for cleaning but the if-elif block was ignoring them and returning None. Ran the cleaning functions and imported in to MongoDB again with Drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " After the second import the address data was audited again and along with the expected uncleaned values like 'West Creek Court 200' was 'Rivercrest Drive South-east'. The DIR_MAPPING dictionary in clean.py was amended to include the mapping for this dirty value. Instead of extracting and importing all data again post_import_clean.py was used to correct it in MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postal Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some postal codes were not in the official format A1B 2C3. The bad postcodes where audited in post_import_audit.py. There were 49 malformed postcodes, 33 of them being unique. A defaultdict was used to produce a dict with keys of the malformed postal codes with blank values. This dict was printed and copied in to the editor and the corrections were done manually since there were an managable number. This took about 5 minutes. This dictionary with the now correct values was moved to post_import_clean.py and a new fuction was created to do the actual updates on my MongoDB instance. After running the cleaning file again the post_import_audit.py was run again and the query to check for malformed postcodes returned no results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rural Roads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset encompassed a very large area and included large parts of farm area around the City of Calgary. As a result there are several rural roads like 'Township Road 204A' which aren't strictly part of the city. Their inclusion did not cause significant issues but they do not add much value to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File Sizes\n",
    "calgary_canada.osm : 159 MB\n",
    "\n",
    "calgary_canada.osm.json : 184 MB\n",
    "\n",
    "##### Number of Documents\n",
    "    db.DANDP3.count() : 863038\n",
    "\n",
    "###### See analyze.py for full code for the following\n",
    "\n",
    "##### Largest Document\n",
    "    print get_largest_doc(get_all_docs(db)) : 112886 characters, 'Proposed West Stoney Trail'\n",
    "\n",
    "##### Number of Unique Users\n",
    "    print len(get_users(docs)) : 767\n",
    "\n",
    "##### Top Three Contributors\n",
    "    print users[0:3] : [{u'count': 309818, u'_id': u'sbrown'}, {u'count': 89769, u'_id': u'Zippanova'}, {u'count': 46296, u'_id': u'markbegbie'}]\n",
    "\n",
    "##### Rank of My Contributions\n",
    "    print 'My Rank ' + mb_rank : 157\n",
    "\n",
    "##### Number of Ways\n",
    "    print 'Number of Ways: ' + str(db.DANDP3.find({\"node_type\":\"way\"}).count()) : 83492\n",
    "\n",
    "##### Number of Nodes\n",
    "    print 'Number of Nodes: ' + str(db.DANDP3.find({\"node_type\":\"node\"}).count()) : 779009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postal Code Standarization\n",
    "During my post import cleaning I made several standardization corrections to the postcodes in the dataset. However, even the correctly formatted postcodes may not be legitimate postcodes. In order to accomplish this, I would have to develop functions that could communicate with the Canada Post address validation API. This would allow me to fully validate whole addresses including the postcodes and then take corrective action. The Canada Post API has the capability to suggest addresses that a closely matching the query submitted. This might pose an interesting challenge to do this autonomously without user intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze User Contributions by Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought it might be interesting to analyze what areas of the city users tend to make contributions. Are there patterns in user contribution that would indicate where that users lives or works? Are there more contributions in newer areas of the city or is it evenly distributed? How much does the City of Calgary contribute? These are just a few questions that I would be interested in digging in to further. To get the most satisfying answers machine learning would have to be implemented to expose these correlations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Additional Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many places in Calgary accept bitcoin?\n",
    "    len(list(db.DANDP3.find({\"payment:bitcoin\" : \"yes\"}))) : 18 \n",
    "\n",
    "##### What are the most common amenties?\n",
    "    amenites = list(aggregate(db, [{'$match' : {'amenity' : {'$exists' : 1}}},\\\n",
    "                               {'$group' : {'_id' : '$amenity', 'count' : \\\n",
    "                               {'$sum' : 1}}}, {'$sort' : {'count' : -1}}]))\n",
    "    print amenites[0:3]\n",
    "    [{u'count': 1735, u'_id': u'parking'}, {u'count': 461, u'_id': u'restaurant'}, {u'count': 441, u'_id': u'fast_food'}]\n",
    "\n",
    "##### What are the most common fast food places?\n",
    "\n",
    "    fast_food_by_name = list(aggregate(db, [{'$match' : {'amenity' : {'$exists' : 1}, 'name' : {'$exists' : 1} }}, {'$group' : {'_id' : '$name', 'count' : {'$sum' : 1}}},{'$sort' : {'count' : -1}}]))\n",
    "    print fast_food_by_name[0:3]\n",
    "    \n",
    "     [{u'_id': u'Tim Hortons', u'count': 59}, {u'_id': u'Subway', u'count': 54}, {u'_id': u'Shell', u'count': 36}]\n",
    "     \n",
    "##### Additional additonal analysis can be found in analyze.py starting at line 111."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working on the Additional Analysis section I discovered that there were some keys that I would have liked to take a closer look but their structure (key:sub_key) made it difficult to get good grouping. This solidified the lesson of audit, clean, repeat for me. Because of the nature of this issue I would have had to go back to the original file processing code and add logic to turn the key:sub_key entries to key : [sub_key : value] entries. For the sake of time I chose not to do this for this project but I want to acknowledge that despite my efforts in cleaning the dataset there is still much more that can be done to improve it. \n",
    "\n",
    "Based on the vast number of contributors, it would seem that people are interested in keeping the map data for Calgary up-to-date. This has advantages and disadvantages for the quality of the data. The more users that contribute the more ways data could be entered incorrectly. Understanding that OpenStreetMaps endeavors to be open, it might be advantageous to intelligently offer suggestions as someone is entering data. This might reduce the number of ill-conceived keys in the data and improve the analysis of map data. \n",
    "\n",
    "While working with OpenStreetMaps data I reflected on what an undertaking it would be to consume these data to enrich a service like Google Maps. A pipeline to ingest these data would have to account for many of the factors that I had to consider plus many more. I have only examined a small fraction of the OpenStreeMaps data and there was a wide range of data quality challenges. I would be interested to see if the techniques I employed are similar to engineers who work on systems that consider ALL OpenStreetMaps data. No doubt, as we produce more and more data as a society we will need to develop systems that assist the data wrangler. For now, we will just have to keep auditing, cleaning, and repeating."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
